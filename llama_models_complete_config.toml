# Complete Llama Models Configuration for TensorZero
# All models configured to use LlamaAPIProvider directly

[gateway]
debug = true

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                              LLAMA MODELS                                  │
# └────────────────────────────────────────────────────────────────────────────┘

# ==============================================================================
# LLAMA 2 MODELS
# ==============================================================================

[models."llama-2-7b-instruct"]
routing = ["llama_api"]

[models."llama-2-7b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-2-7B-Instruct"

[models."llama-2-7b-chat"]
routing = ["llama_api"]

[models."llama-2-7b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-2-7B-Chat"

[models."llama-2-13b-instruct"]
routing = ["llama_api"]

[models."llama-2-13b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-2-13B-Instruct"

[models."llama-2-13b-chat"]
routing = ["llama_api"]

[models."llama-2-13b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-2-13B-Chat"

[models."llama-2-70b-instruct"]
routing = ["llama_api"]

[models."llama-2-70b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-2-70B-Instruct"

[models."llama-2-70b-chat"]
routing = ["llama_api"]

[models."llama-2-70b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-2-70B-Chat"

# ==============================================================================
# LLAMA 3 MODELS
# ==============================================================================

[models."llama-3-8b-instruct"]
routing = ["llama_api"]

[models."llama-3-8b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3-8B-Instruct"

[models."llama-3-8b-chat"]
routing = ["llama_api"]

[models."llama-3-8b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-3-8B-Chat"

[models."llama-3-70b-instruct"]
routing = ["llama_api"]

[models."llama-3-70b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3-70B-Instruct"

[models."llama-3-70b-chat"]
routing = ["llama_api"]

[models."llama-3-70b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-3-70B-Chat"

# ==============================================================================
# LLAMA 3.1 MODELS
# ==============================================================================

[models."llama-3.1-8b-instruct"]
routing = ["llama_api"]

[models."llama-3.1-8b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.1-8B-Instruct"

[models."llama-3.1-8b-chat"]
routing = ["llama_api"]

[models."llama-3.1-8b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.1-8B-Chat"

[models."llama-3.1-70b-instruct"]
routing = ["llama_api"]

[models."llama-3.1-70b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.1-70B-Instruct"

[models."llama-3.1-70b-chat"]
routing = ["llama_api"]

[models."llama-3.1-70b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.1-70B-Chat"

[models."llama-3.1-405b-instruct"]
routing = ["llama_api"]

[models."llama-3.1-405b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.1-405B-Instruct"

[models."llama-3.1-405b-chat"]
routing = ["llama_api"]

[models."llama-3.1-405b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.1-405B-Chat"

# ==============================================================================
# LLAMA 3.2 MODELS
# ==============================================================================

[models."llama-3.2-1b-instruct"]
routing = ["llama_api"]

[models."llama-3.2-1b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.2-1B-Instruct"

[models."llama-3.2-1b-chat"]
routing = ["llama_api"]

[models."llama-3.2-1b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.2-1B-Chat"

[models."llama-3.2-3b-instruct"]
routing = ["llama_api"]

[models."llama-3.2-3b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.2-3B-Instruct"

[models."llama-3.2-3b-chat"]
routing = ["llama_api"]

[models."llama-3.2-3b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.2-3B-Chat"

# ==============================================================================
# LLAMA 3.2 VISION MODELS
# ==============================================================================

[models."llama-3.2-vision-11b-instruct"]
routing = ["llama_api"]

[models."llama-3.2-vision-11b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.2-Vision-11B-Instruct"

[models."llama-3.2-vision-11b-chat"]
routing = ["llama_api"]

[models."llama-3.2-vision-11b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.2-Vision-11B-Chat"

[models."llama-3.2-vision-90b-instruct"]
routing = ["llama_api"]

[models."llama-3.2-vision-90b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.2-Vision-90B-Instruct"

[models."llama-3.2-vision-90b-chat"]
routing = ["llama_api"]

[models."llama-3.2-vision-90b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.2-Vision-90B-Chat"

# ==============================================================================
# LLAMA 3.3 MODELS
# ==============================================================================

[models."llama-3.3-70b-instruct"]
routing = ["llama_api"]

[models."llama-3.3-70b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.3-70B-Instruct"

[models."llama-3.3-70b-chat"]
routing = ["llama_api"]

[models."llama-3.3-70b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.3-70B-Chat"

# ==============================================================================
# LLAMA 4 MODELS
# ==============================================================================

[models."llama-4-scout-17b-instruct"]
routing = ["llama_api"]

[models."llama-4-scout-17b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-4-Scout-17B-16E-Instruct-FP8"

[models."llama-4-scout-17b-chat"]
routing = ["llama_api"]

[models."llama-4-scout-17b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-4-Scout-17B-16E-Chat-FP8"

[models."llama-4-maverick-17b-instruct"]
routing = ["llama_api"]

[models."llama-4-maverick-17b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-4-Maverick-17B-128E-Instruct-FP8"

[models."llama-4-maverick-17b-chat"]
routing = ["llama_api"]

[models."llama-4-maverick-17b-chat".providers.llama_api]
type = "llama_api"
model_name = "Llama-4-Maverick-17B-128E-Chat-FP8"

# ==============================================================================
# DYNAMIC API KEY CONFIGURATIONS
# ==============================================================================

# Example dynamic configurations for each model family
[models."llama-2-7b-instruct-dynamic"]
routing = ["llama_api"]

[models."llama-2-7b-instruct-dynamic".providers.llama_api]
type = "llama_api"
model_name = "Llama-2-7B-Instruct"
api_key_location = "dynamic::llama_api_key"

[models."llama-3.1-8b-instruct-dynamic"]
routing = ["llama_api"]

[models."llama-3.1-8b-instruct-dynamic".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.1-8B-Instruct"
api_key_location = "dynamic::llama_api_key"

[models."llama-4-scout-17b-instruct-dynamic"]
routing = ["llama_api"]

[models."llama-4-scout-17b-instruct-dynamic".providers.llama_api]
type = "llama_api"
model_name = "Llama-4-Scout-17B-16E-Instruct-FP8"
api_key_location = "dynamic::llama_api_key"

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                 FUNCTIONS                                  │
# └────────────────────────────────────────────────────────────────────────────┘

# Example function using Llama models
[functions.llama_chat_example]
type = "chat"

[functions.llama_chat_example.variants.llama_2_7b]
type = "chat_completion"
model = "llama-2-7b-instruct"

[functions.llama_chat_example.variants.llama_3_8b]
type = "chat_completion"
model = "llama-3-8b-instruct"

[functions.llama_chat_example.variants.llama_3_1_8b]
type = "chat_completion"
model = "llama-3.1-8b-instruct"

[functions.llama_chat_example.variants.llama_4_scout]
type = "chat_completion"
model = "llama-4-scout-17b-instruct"

# Example JSON function using Llama
[functions.llama_json_example]
type = "json"
output_schema = "functions/llama_json_example/output_schema.json"

[functions.llama_json_example.variants.llama_3_1_8b]
type = "chat_completion"
model = "llama-3.1-8b-instruct"

# Example vision function using Llama 3.2 Vision
[functions.llama_vision_example]
type = "chat"

[functions.llama_vision_example.variants.llama_3_2_vision_11b]
type = "chat_completion"
model = "llama-3.2-vision-11b-instruct"
