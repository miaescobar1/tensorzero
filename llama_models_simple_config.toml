# Simple Llama Models Configuration for TensorZero
# All models configured to use LlamaAPIProvider directly

[gateway]
debug = true

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                              LLAMA MODELS                                  │
# └────────────────────────────────────────────────────────────────────────────┘

# ==============================================================================
# LLAMA 2 MODELS
# ==============================================================================

[models."llama-2-7b-instruct"]
routing = ["llama_api"]

[models."llama-2-7b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-2-7B-Instruct"

[models."llama-2-13b-instruct"]
routing = ["llama_api"]

[models."llama-2-13b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-2-13B-Instruct"

[models."llama-2-70b-instruct"]
routing = ["llama_api"]

[models."llama-2-70b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-2-70B-Instruct"

# ==============================================================================
# LLAMA 3 MODELS
# ==============================================================================

[models."llama-3-8b-instruct"]
routing = ["llama_api"]

[models."llama-3-8b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3-8B-Instruct"

[models."llama-3-70b-instruct"]
routing = ["llama_api"]

[models."llama-3-70b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3-70B-Instruct"

# ==============================================================================
# LLAMA 3.1 MODELS
# ==============================================================================

[models."llama-3.1-8b-instruct"]
routing = ["llama_api"]

[models."llama-3.1-8b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.1-8B-Instruct"

[models."llama-3.1-70b-instruct"]
routing = ["llama_api"]

[models."llama-3.1-70b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.1-70B-Instruct"

[models."llama-3.1-405b-instruct"]
routing = ["llama_api"]

[models."llama-3.1-405b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-3.1-405B-Instruct"

# ==============================================================================
# LLAMA 4 MODELS
# ==============================================================================

[models."llama-4-scout-17b-instruct"]
routing = ["llama_api"]

[models."llama-4-scout-17b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-4-Scout-17B-16E-Instruct-FP8"

[models."llama-4-maverick-17b-instruct"]
routing = ["llama_api"]

[models."llama-4-maverick-17b-instruct".providers.llama_api]
type = "llama_api"
model_name = "Llama-4-Maverick-17B-128E-Instruct-FP8"

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                 FUNCTIONS                                  │
# └────────────────────────────────────────────────────────────────────────────┘

# Simple chat function using Llama models
[functions.llama_chat_test]
type = "chat"

[functions.llama_chat_test.variants.llama_2_7b]
type = "chat_completion"
model = "llama-2-7b-instruct"

[functions.llama_chat_test.variants.llama_3_8b]
type = "chat_completion"
model = "llama-3-8b-instruct"

[functions.llama_chat_test.variants.llama_3_1_8b]
type = "chat_completion"
model = "llama-3.1-8b-instruct"

[functions.llama_chat_test.variants.llama_4_scout]
type = "chat_completion"
model = "llama-4-scout-17b-instruct"
